{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_urls_of_type\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import requests\n",
    "import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {\n",
    "             'thoi-su': ['chinh-tri', 'dan-sinh', 'lao-dong-viec-lam', 'giao-thong', 'mekong', 'quy-hy-vong'],\n",
    "             'goc-nhin': ['binh-luan-nhieu', 'chinh-tri-chinh-sach', 'y-te-suc-khoe', 'kinh-doanh-quan-tri', 'giao-duc-tri-thuc', 'moi-truong', 'van-hoa-doi-song', 'covid-19', 'tac-gia'],\n",
    "             'the-gioi': ['bau-cu-tong-thong-my', 'tu-lieu', 'phan-tich', 'nguoi-viet-5-chau', 'cuoc-song-do-day', 'quan-su'],\n",
    "             'kinh-doanh': ['net-zero', 'quoc-te', 'doanh-nghiep', 'chung-khoan', 'ebank', 'vi-mo', 'tien-cua-toi', 'hang-hoa'],\n",
    "             'bat-dong-san': ['chinh-sach', 'thi-truong', 'khong-gian-song', 'tu-van'],\n",
    "             'khoa-hoc': ['khoa-hoc-trong-nuoc', 'pii-doi-moi-sang-tao', 'tin-tuc', 'phat-minh', 'ung-dung', 'the-gioi-tu-nhien', 'thuong-thuc'],\n",
    "             'giai-tri': ['gioi-sao', 'sach', 'video', 'phim', 'nhac', 'thoi-trang', 'lam-dep', 'san-khau-my-thuat'],\n",
    "             'the-thao': ['bong-da', 'du-lieu-bong-da', 'marathon', 'tennis', 'cac-mon-khac', 'hau-truong'],\n",
    "             'phap-luat': ['ho-so-pha-an', 'tu-van'],\n",
    "             'giao-duc': ['tin-tuc', 'tuyen-sinh', 'chan-dung', 'du-hoc', 'thao-luan', 'hoc-tieng-anh', 'giao-duc-40'],\n",
    "             'suc-khoe': ['tin-tuc', 'cac-benh', 'song-khoe', 'vaccine'],\n",
    "             'doi-song': ['nhip-song', 'to-am', 'bai-hoc-song', 'cooking', 'tieu-dung'],\n",
    "             'du-lich': ['diem-den', 'am-thuc', 'dau-chan', 'tu-van', 'cam-nang'],\n",
    "             'so-hoa': ['cong-nghe', 'san-pham', 'blockchain'],\n",
    "             'xe': ['thi-truong', 'dien-dan'],\n",
    "             'y-kien': ['thoi-su', 'doi-song']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/urls\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_url_list = []\n",
    "f_urls = open(\"data/urls/testurl_list.jsonl\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "total_count = 0 \n",
    "numlost = 0\n",
    "max_numlost = 3\n",
    "\n",
    "for type in tqdm.tqdm(type_dict, desc=\"Processing types\"):\n",
    "    for sub_type in tqdm.tqdm(type_dict[type], desc=f\"Processing subtypes for {type}\"):\n",
    "        target_count = 5\n",
    "        i = 1\n",
    "        current_count = 0\n",
    "        no_articles_in_a_row = 0\n",
    "        \n",
    "        while current_count < target_count:\n",
    "            url = f\"https://vnexpress.net/{type}/{sub_type}-p{i}\"\n",
    "            content = requests.get(url)\n",
    "\n",
    "            if content.status_code != 200:\n",
    "                print(f\"{content.status_code} ERROR {type}|{sub_type}\")\n",
    "                break\n",
    "            \n",
    "            soup = BeautifulSoup(content.content, \"html.parser\")\n",
    "            tmp_title_list = soup.find_all(class_=\"title-news\")\n",
    "            \n",
    "            if len(tmp_title_list) == 0:\n",
    "                no_articles_in_a_row += 1\n",
    "                print(f\"No article {type}|{sub_type}.\")\n",
    "                break\n",
    "            else:\n",
    "                no_articles_in_a_row = 0\n",
    "\n",
    "                for title in tmp_title_list:\n",
    "                    try:\n",
    "                        article_info = title.find_all(\"a\")\n",
    "                        date_time = soup.find_all(\"span\", class_=\"date\")\n",
    "                        \n",
    "                        if article_info:\n",
    "                            article_url = article_info[0].get(\"href\")\n",
    "                            article_title = article_info[0].get(\"title\")\n",
    "                            description = title.find_next(\"p\", class_=\"description\") \n",
    "                            description_text = \"null\"\n",
    "\n",
    "                            if description:\n",
    "                                description_text = description.get_text(strip=True)\n",
    "\n",
    "                            sample = {\n",
    "                                \"url\": article_url,\n",
    "                                \"title\": article_title,\n",
    "                                \"type\": f\"{type}|{sub_type}\",\n",
    "                                \"description\": description_text\n",
    "                            }\n",
    "                            f_urls.write(json.dumps(sample, indent=4, ensure_ascii=False) + \"\\n\")\n",
    "                            current_count += 1\n",
    "                            total_count += 1\n",
    "\n",
    "                            if current_count % 5 == 0:\n",
    "                                print(f\"Total: {total_count}\")\n",
    "\n",
    "                            if current_count >= target_count:\n",
    "                                break\n",
    "                            \n",
    "                            numlost = 0\n",
    "                            \n",
    "                        else:\n",
    "                            numlost += 1\n",
    "                            print(f\"No <a> tag for {type}|{sub_type}. Continue...\")\n",
    "                            if numlost >= max_numlost:\n",
    "                                print(f\"No <a> tag for {type}|{sub_type}. Exiting...\")\n",
    "                                break\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: {e}\")\n",
    "                        pass\n",
    "            \n",
    "            if numlost >= max_numlost:\n",
    "                print(f\"No <a> tag for {type}|{sub_type}. Exiting....\")\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if numlost >= max_numlost:\n",
    "            print(f\"No <a> tag for {type}|{sub_type}. Exiting.....\")\n",
    "            break\n",
    "\n",
    "f_urls.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
